from safetensors.torch import load_file, save_file
import torch
import argparse

parser = argparse.ArgumentParser(description="Processes Diffusers 0.6.x VAEs into Diffusers 0.11.1 VAEs")
parser.add_argument("--input", help="The file path for the imput.", required=True)
parser.add_argument("--out", help="The file path for the output.", required=True)
args = parser.parse_args()

old_vae = load_file(args.input)
vae_keys = list(old_vae.keys())

# I hate this as much as you do - but there isn't a realistic alternative
patch_list = {
	"decoder.norm_out.bias":   "decoder.conv_norm_out.bias",
	"decoder.norm_out.weight": "decoder.conv_norm_out.weight",

	"decoder.mid.attn_1.norm.bias":       "decoder.mid_block.attentions.0.group_norm.bias",
	"decoder.mid.attn_1.norm.weight":     "decoder.mid_block.attentions.0.group_norm.weight",
	"decoder.mid.attn_1.k.bias":          "decoder.mid_block.attentions.0.key.bias",
	"decoder.mid.attn_1.k.weight":        "decoder.mid_block.attentions.0.key.weight",
	"decoder.mid.attn_1.proj_out.bias":   "decoder.mid_block.attentions.0.proj_attn.bias",
	"decoder.mid.attn_1.proj_out.weight": "decoder.mid_block.attentions.0.proj_attn.weight",
	"decoder.mid.attn_1.q.bias":          "decoder.mid_block.attentions.0.query.bias",
	"decoder.mid.attn_1.q.weight":        "decoder.mid_block.attentions.0.query.weight",
	"decoder.mid.attn_1.v.bias":          "decoder.mid_block.attentions.0.value.bias",
	"decoder.mid.attn_1.v.weight":        "decoder.mid_block.attentions.0.value.weight",

	"decoder.mid.block_1.conv1.bias":   "decoder.mid_block.resnets.0.conv1.bias",
	"decoder.mid.block_1.conv1.weight": "decoder.mid_block.resnets.0.conv1.weight",
	"decoder.mid.block_1.conv2.bias":   "decoder.mid_block.resnets.0.conv2.bias",
	"decoder.mid.block_1.conv2.weight": "decoder.mid_block.resnets.0.conv2.weight",
	"decoder.mid.block_1.norm1.bias":   "decoder.mid_block.resnets.0.norm1.bias",
	"decoder.mid.block_1.norm1.weight": "decoder.mid_block.resnets.0.norm1.weight",
	"decoder.mid.block_1.norm2.bias":   "decoder.mid_block.resnets.0.norm2.bias",
	"decoder.mid.block_1.norm2.weight": "decoder.mid_block.resnets.0.norm2.weight",

	"decoder.mid.block_2.conv1.bias": "decoder.mid_block.resnets.1.conv1.bias",
	"decoder.mid.block_2.conv1.weight": "decoder.mid_block.resnets.1.conv1.weight",
	"decoder.mid.block_2.conv2.bias": "decoder.mid_block.resnets.1.conv2.bias",
	"decoder.mid.block_2.conv2.weight": "decoder.mid_block.resnets.1.conv2.weight",
	"decoder.mid.block_2.norm1.bias": "decoder.mid_block.resnets.1.norm1.bias",
	"decoder.mid.block_2.norm1.weight": "decoder.mid_block.resnets.1.norm1.weight",
	"decoder.mid.block_2.norm2.bias": "decoder.mid_block.resnets.1.norm2.bias",
	"decoder.mid.block_2.norm2.weight": "decoder.mid_block.resnets.1.norm2.weight",

	"decoder.up.0.block.0.nin_shortcut.bias":   "decoder.up_blocks.3.resnets.0.conv_shortcut.bias",
	"decoder.up.0.block.0.nin_shortcut.weight": "decoder.up_blocks.3.resnets.0.conv_shortcut.weight",
	"decoder.up.0.block.0.conv1.bias":          "decoder.up_blocks.3.resnets.0.conv1.bias",
	"decoder.up.0.block.0.conv1.weight":        "decoder.up_blocks.3.resnets.0.conv1.weight",
	"decoder.up.0.block.0.conv2.bias":          "decoder.up_blocks.3.resnets.0.conv2.bias",
	"decoder.up.0.block.0.conv2.weight":        "decoder.up_blocks.3.resnets.0.conv2.weight",
	"decoder.up.0.block.0.norm1.bias":          "decoder.up_blocks.3.resnets.0.norm1.bias",
	"decoder.up.0.block.0.norm1.weight":        "decoder.up_blocks.3.resnets.0.norm1.weight",
	"decoder.up.0.block.0.norm2.bias":          "decoder.up_blocks.3.resnets.0.norm2.bias",
	"decoder.up.0.block.0.norm2.weight":        "decoder.up_blocks.3.resnets.0.norm2.weight",
	"decoder.up.0.block.1.conv1.bias":          "decoder.up_blocks.3.resnets.1.conv1.bias",
	"decoder.up.0.block.1.conv1.weight":        "decoder.up_blocks.3.resnets.1.conv1.weight",
	"decoder.up.0.block.1.conv2.bias":          "decoder.up_blocks.3.resnets.1.conv2.bias",
	"decoder.up.0.block.1.conv2.weight":        "decoder.up_blocks.3.resnets.1.conv2.weight",
	"decoder.up.0.block.1.norm1.bias":          "decoder.up_blocks.3.resnets.1.norm1.bias",
	"decoder.up.0.block.1.norm1.weight":        "decoder.up_blocks.3.resnets.1.norm1.weight",
	"decoder.up.0.block.1.norm2.bias":          "decoder.up_blocks.3.resnets.1.norm2.bias",
	"decoder.up.0.block.1.norm2.weight":        "decoder.up_blocks.3.resnets.1.norm2.weight",
	"decoder.up.0.block.2.conv1.bias":          "decoder.up_blocks.3.resnets.2.conv1.bias",
	"decoder.up.0.block.2.conv1.weight":        "decoder.up_blocks.3.resnets.2.conv1.weight",
	"decoder.up.0.block.2.conv2.bias":          "decoder.up_blocks.3.resnets.2.conv2.bias",
	"decoder.up.0.block.2.conv2.weight":        "decoder.up_blocks.3.resnets.2.conv2.weight",
	"decoder.up.0.block.2.norm1.bias":          "decoder.up_blocks.3.resnets.2.norm1.bias",
	"decoder.up.0.block.2.norm1.weight":        "decoder.up_blocks.3.resnets.2.norm1.weight",
	"decoder.up.0.block.2.norm2.bias":          "decoder.up_blocks.3.resnets.2.norm2.bias",
	"decoder.up.0.block.2.norm2.weight":        "decoder.up_blocks.3.resnets.2.norm2.weight",

	"decoder.up.1.block.0.nin_shortcut.bias": "decoder.up_blocks.2.resnets.0.conv_shortcut.bias",
	"decoder.up.1.block.0.nin_shortcut.weight": "decoder.up_blocks.2.resnets.0.conv_shortcut.weight",
	"decoder.up.1.upsample.conv.bias":   "decoder.up_blocks.2.upsamplers.0.conv.bias",
	"decoder.up.1.upsample.conv.weight": "decoder.up_blocks.2.upsamplers.0.conv.weight",
	"decoder.up.1.block.0.conv1.bias":   "decoder.up_blocks.2.resnets.0.conv1.bias",
	"decoder.up.1.block.0.conv1.weight": "decoder.up_blocks.2.resnets.0.conv1.weight",
	"decoder.up.1.block.0.conv2.bias":   "decoder.up_blocks.2.resnets.0.conv2.bias",
	"decoder.up.1.block.0.conv2.weight": "decoder.up_blocks.2.resnets.0.conv2.weight",
	"decoder.up.1.block.0.norm1.bias":   "decoder.up_blocks.2.resnets.0.norm1.bias",
	"decoder.up.1.block.0.norm1.weight": "decoder.up_blocks.2.resnets.0.norm1.weight",
	"decoder.up.1.block.0.norm2.bias":   "decoder.up_blocks.2.resnets.0.norm2.bias",
	"decoder.up.1.block.0.norm2.weight": "decoder.up_blocks.2.resnets.0.norm2.weight",
	"decoder.up.1.block.1.conv1.bias":   "decoder.up_blocks.2.resnets.1.conv1.bias",
	"decoder.up.1.block.1.conv1.weight": "decoder.up_blocks.2.resnets.1.conv1.weight",
	"decoder.up.1.block.1.conv2.bias":   "decoder.up_blocks.2.resnets.1.conv2.bias",
	"decoder.up.1.block.1.conv2.weight": "decoder.up_blocks.2.resnets.1.conv2.weight",
	"decoder.up.1.block.1.norm1.bias":   "decoder.up_blocks.2.resnets.1.norm1.bias",
	"decoder.up.1.block.1.norm1.weight": "decoder.up_blocks.2.resnets.1.norm1.weight",
	"decoder.up.1.block.1.norm2.bias":   "decoder.up_blocks.2.resnets.1.norm2.bias",
	"decoder.up.1.block.1.norm2.weight": "decoder.up_blocks.2.resnets.1.norm2.weight",
	"decoder.up.1.block.2.conv1.bias":   "decoder.up_blocks.2.resnets.2.conv1.bias",
	"decoder.up.1.block.2.conv1.weight": "decoder.up_blocks.2.resnets.2.conv1.weight",
	"decoder.up.1.block.2.conv2.bias":   "decoder.up_blocks.2.resnets.2.conv2.bias",
	"decoder.up.1.block.2.conv2.weight": "decoder.up_blocks.2.resnets.2.conv2.weight",
	"decoder.up.1.block.2.norm1.bias":   "decoder.up_blocks.2.resnets.2.norm1.bias",
	"decoder.up.1.block.2.norm1.weight": "decoder.up_blocks.2.resnets.2.norm1.weight",
	"decoder.up.1.block.2.norm2.bias":   "decoder.up_blocks.2.resnets.2.norm2.bias",
	"decoder.up.1.block.2.norm2.weight": "decoder.up_blocks.2.resnets.2.norm2.weight",

	"decoder.up.2.upsample.conv.bias":   "decoder.up_blocks.1.upsamplers.0.conv.bias",
	"decoder.up.2.upsample.conv.weight": "decoder.up_blocks.1.upsamplers.0.conv.weight",
	"decoder.up.2.block.0.conv1.bias":   "decoder.up_blocks.1.resnets.0.conv1.bias",
	"decoder.up.2.block.0.conv1.weight": "decoder.up_blocks.1.resnets.0.conv1.weight",
	"decoder.up.2.block.0.conv2.bias":   "decoder.up_blocks.1.resnets.0.conv2.bias",
	"decoder.up.2.block.0.conv2.weight": "decoder.up_blocks.1.resnets.0.conv2.weight",
	"decoder.up.2.block.0.norm1.bias":   "decoder.up_blocks.1.resnets.0.norm1.bias",
	"decoder.up.2.block.0.norm1.weight": "decoder.up_blocks.1.resnets.0.norm1.weight",
	"decoder.up.2.block.0.norm2.bias":   "decoder.up_blocks.1.resnets.0.norm2.bias",
	"decoder.up.2.block.0.norm2.weight": "decoder.up_blocks.1.resnets.0.norm2.weight",
	"decoder.up.2.block.1.conv1.bias":   "decoder.up_blocks.1.resnets.1.conv1.bias",
	"decoder.up.2.block.1.conv1.weight": "decoder.up_blocks.1.resnets.1.conv1.weight",
	"decoder.up.2.block.1.conv2.bias":   "decoder.up_blocks.1.resnets.1.conv2.bias",
	"decoder.up.2.block.1.conv2.weight": "decoder.up_blocks.1.resnets.1.conv2.weight",
	"decoder.up.2.block.1.norm1.bias":   "decoder.up_blocks.1.resnets.1.norm1.bias",
	"decoder.up.2.block.1.norm1.weight": "decoder.up_blocks.1.resnets.1.norm1.weight",
	"decoder.up.2.block.1.norm2.bias":   "decoder.up_blocks.1.resnets.1.norm2.bias",
	"decoder.up.2.block.1.norm2.weight": "decoder.up_blocks.1.resnets.1.norm2.weight",
	"decoder.up.2.block.2.conv1.bias":   "decoder.up_blocks.1.resnets.2.conv1.bias",
	"decoder.up.2.block.2.conv1.weight": "decoder.up_blocks.1.resnets.2.conv1.weight",
	"decoder.up.2.block.2.conv2.bias":   "decoder.up_blocks.1.resnets.2.conv2.bias",
	"decoder.up.2.block.2.conv2.weight": "decoder.up_blocks.1.resnets.2.conv2.weight",
	"decoder.up.2.block.2.norm1.bias":   "decoder.up_blocks.1.resnets.2.norm1.bias",
	"decoder.up.2.block.2.norm1.weight": "decoder.up_blocks.1.resnets.2.norm1.weight",
	"decoder.up.2.block.2.norm2.bias":   "decoder.up_blocks.1.resnets.2.norm2.bias",
	"decoder.up.2.block.2.norm2.weight": "decoder.up_blocks.1.resnets.2.norm2.weight",

	"decoder.up.3.upsample.conv.bias":   "decoder.up_blocks.0.upsamplers.0.conv.bias",
	"decoder.up.3.upsample.conv.weight": "decoder.up_blocks.0.upsamplers.0.conv.weight",
	"decoder.up.3.block.0.conv1.bias":   "decoder.up_blocks.0.resnets.0.conv1.bias",
	"decoder.up.3.block.0.conv1.weight": "decoder.up_blocks.0.resnets.0.conv1.weight",
	"decoder.up.3.block.0.conv2.bias":   "decoder.up_blocks.0.resnets.0.conv2.bias",
	"decoder.up.3.block.0.conv2.weight": "decoder.up_blocks.0.resnets.0.conv2.weight",
	"decoder.up.3.block.0.norm1.bias":   "decoder.up_blocks.0.resnets.0.norm1.bias",
	"decoder.up.3.block.0.norm1.weight": "decoder.up_blocks.0.resnets.0.norm1.weight",
	"decoder.up.3.block.0.norm2.bias":   "decoder.up_blocks.0.resnets.0.norm2.bias",
	"decoder.up.3.block.0.norm2.weight": "decoder.up_blocks.0.resnets.0.norm2.weight",
	"decoder.up.3.block.1.conv1.bias":   "decoder.up_blocks.0.resnets.1.conv1.bias",
	"decoder.up.3.block.1.conv1.weight": "decoder.up_blocks.0.resnets.1.conv1.weight",
	"decoder.up.3.block.1.conv2.bias":   "decoder.up_blocks.0.resnets.1.conv2.bias",
	"decoder.up.3.block.1.conv2.weight": "decoder.up_blocks.0.resnets.1.conv2.weight",
	"decoder.up.3.block.1.norm1.bias":   "decoder.up_blocks.0.resnets.1.norm1.bias",
	"decoder.up.3.block.1.norm1.weight": "decoder.up_blocks.0.resnets.1.norm1.weight",
	"decoder.up.3.block.1.norm2.bias":   "decoder.up_blocks.0.resnets.1.norm2.bias",
	"decoder.up.3.block.1.norm2.weight": "decoder.up_blocks.0.resnets.1.norm2.weight",
	"decoder.up.3.block.2.conv1.bias":   "decoder.up_blocks.0.resnets.2.conv1.bias",
	"decoder.up.3.block.2.conv1.weight": "decoder.up_blocks.0.resnets.2.conv1.weight",
	"decoder.up.3.block.2.conv2.bias":   "decoder.up_blocks.0.resnets.2.conv2.bias",
	"decoder.up.3.block.2.conv2.weight": "decoder.up_blocks.0.resnets.2.conv2.weight",
	"decoder.up.3.block.2.norm1.bias":   "decoder.up_blocks.0.resnets.2.norm1.bias",
	"decoder.up.3.block.2.norm1.weight": "decoder.up_blocks.0.resnets.2.norm1.weight",
	"decoder.up.3.block.2.norm2.bias":   "decoder.up_blocks.0.resnets.2.norm2.bias",
	"decoder.up.3.block.2.norm2.weight": "decoder.up_blocks.0.resnets.2.norm2.weight",

	"encoder.norm_out.bias": "encoder.conv_norm_out.bias",
	"encoder.norm_out.weight": "encoder.conv_norm_out.weight",

	"encoder.down.0.downsample.conv.bias":   "encoder.down_blocks.0.downsamplers.0.conv.bias",
	"encoder.down.0.downsample.conv.weight": "encoder.down_blocks.0.downsamplers.0.conv.weight",
	"encoder.down.0.block.0.conv1.bias":   "encoder.down_blocks.0.resnets.0.conv1.bias",
	"encoder.down.0.block.0.conv1.weight": "encoder.down_blocks.0.resnets.0.conv1.weight",
	"encoder.down.0.block.0.conv2.bias":   "encoder.down_blocks.0.resnets.0.conv2.bias",
	"encoder.down.0.block.0.conv2.weight": "encoder.down_blocks.0.resnets.0.conv2.weight",
	"encoder.down.0.block.0.norm1.bias":   "encoder.down_blocks.0.resnets.0.norm1.bias",
	"encoder.down.0.block.0.norm1.weight": "encoder.down_blocks.0.resnets.0.norm1.weight",
	"encoder.down.0.block.0.norm2.bias":   "encoder.down_blocks.0.resnets.0.norm2.bias",
	"encoder.down.0.block.0.norm2.weight": "encoder.down_blocks.0.resnets.0.norm2.weight",
	"encoder.down.0.block.1.conv1.bias":   "encoder.down_blocks.0.resnets.1.conv1.bias",
	"encoder.down.0.block.1.conv1.weight": "encoder.down_blocks.0.resnets.1.conv1.weight",
	"encoder.down.0.block.1.conv2.bias":   "encoder.down_blocks.0.resnets.1.conv2.bias",
	"encoder.down.0.block.1.conv2.weight": "encoder.down_blocks.0.resnets.1.conv2.weight",
	"encoder.down.0.block.1.norm1.bias":   "encoder.down_blocks.0.resnets.1.norm1.bias",
	"encoder.down.0.block.1.norm1.weight": "encoder.down_blocks.0.resnets.1.norm1.weight",
	"encoder.down.0.block.1.norm2.bias":   "encoder.down_blocks.0.resnets.1.norm2.bias",
	"encoder.down.0.block.1.norm2.weight": "encoder.down_blocks.0.resnets.1.norm2.weight",

	"encoder.down.1.downsample.conv.bias":        "encoder.down_blocks.1.downsamplers.0.conv.bias",
	"encoder.down.1.downsample.conv.weight":      "encoder.down_blocks.1.downsamplers.0.conv.weight",
	"encoder.down.1.block.0.nin_shortcut.bias":   "encoder.down_blocks.1.resnets.0.conv_shortcut.bias",
	"encoder.down.1.block.0.nin_shortcut.weight": "encoder.down_blocks.1.resnets.0.conv_shortcut.weight",
	"encoder.down.1.block.0.conv1.bias":   "encoder.down_blocks.1.resnets.0.conv1.bias",
	"encoder.down.1.block.0.conv1.weight": "encoder.down_blocks.1.resnets.0.conv1.weight",
	"encoder.down.1.block.0.conv2.bias":   "encoder.down_blocks.1.resnets.0.conv2.bias",
	"encoder.down.1.block.0.conv2.weight": "encoder.down_blocks.1.resnets.0.conv2.weight",
	"encoder.down.1.block.0.norm1.bias":   "encoder.down_blocks.1.resnets.0.norm1.bias",
	"encoder.down.1.block.0.norm1.weight": "encoder.down_blocks.1.resnets.0.norm1.weight",
	"encoder.down.1.block.0.norm2.bias":   "encoder.down_blocks.1.resnets.0.norm2.bias",
	"encoder.down.1.block.0.norm2.weight": "encoder.down_blocks.1.resnets.0.norm2.weight",
	"encoder.down.1.block.1.conv1.bias":   "encoder.down_blocks.1.resnets.1.conv1.bias",
	"encoder.down.1.block.1.conv1.weight": "encoder.down_blocks.1.resnets.1.conv1.weight",
	"encoder.down.1.block.1.conv2.bias":   "encoder.down_blocks.1.resnets.1.conv2.bias",
	"encoder.down.1.block.1.conv2.weight": "encoder.down_blocks.1.resnets.1.conv2.weight",
	"encoder.down.1.block.1.norm1.bias":   "encoder.down_blocks.1.resnets.1.norm1.bias",
	"encoder.down.1.block.1.norm1.weight": "encoder.down_blocks.1.resnets.1.norm1.weight",
	"encoder.down.1.block.1.norm2.bias":   "encoder.down_blocks.1.resnets.1.norm2.bias",
	"encoder.down.1.block.1.norm2.weight": "encoder.down_blocks.1.resnets.1.norm2.weight",

	"encoder.down.2.downsample.conv.bias":        "encoder.down_blocks.2.downsamplers.0.conv.bias",
	"encoder.down.2.downsample.conv.weight":      "encoder.down_blocks.2.downsamplers.0.conv.weight",
	"encoder.down.2.block.0.nin_shortcut.bias":   "encoder.down_blocks.2.resnets.0.conv_shortcut.bias",
	"encoder.down.2.block.0.nin_shortcut.weight": "encoder.down_blocks.2.resnets.0.conv_shortcut.weight",
	"encoder.down.2.block.0.conv1.bias":   "encoder.down_blocks.2.resnets.0.conv1.bias",
	"encoder.down.2.block.0.conv1.weight": "encoder.down_blocks.2.resnets.0.conv1.weight",
	"encoder.down.2.block.0.conv2.bias":   "encoder.down_blocks.2.resnets.0.conv2.bias",
	"encoder.down.2.block.0.conv2.weight": "encoder.down_blocks.2.resnets.0.conv2.weight",
	"encoder.down.2.block.0.norm1.bias":   "encoder.down_blocks.2.resnets.0.norm1.bias",
	"encoder.down.2.block.0.norm1.weight": "encoder.down_blocks.2.resnets.0.norm1.weight",
	"encoder.down.2.block.0.norm2.bias":   "encoder.down_blocks.2.resnets.0.norm2.bias",
	"encoder.down.2.block.0.norm2.weight": "encoder.down_blocks.2.resnets.0.norm2.weight",
	"encoder.down.2.block.1.conv1.bias":   "encoder.down_blocks.2.resnets.1.conv1.bias",
	"encoder.down.2.block.1.conv1.weight": "encoder.down_blocks.2.resnets.1.conv1.weight",
	"encoder.down.2.block.1.conv2.bias":   "encoder.down_blocks.2.resnets.1.conv2.bias",
	"encoder.down.2.block.1.conv2.weight": "encoder.down_blocks.2.resnets.1.conv2.weight",
	"encoder.down.2.block.1.norm1.bias":   "encoder.down_blocks.2.resnets.1.norm1.bias",
	"encoder.down.2.block.1.norm1.weight": "encoder.down_blocks.2.resnets.1.norm1.weight",
	"encoder.down.2.block.1.norm2.bias":   "encoder.down_blocks.2.resnets.1.norm2.bias",
	"encoder.down.2.block.1.norm2.weight": "encoder.down_blocks.2.resnets.1.norm2.weight",

	"encoder.down.3.block.0.conv1.bias": "encoder.down_blocks.3.resnets.0.conv1.bias",
	"encoder.down.3.block.0.conv1.weight": "encoder.down_blocks.3.resnets.0.conv1.weight",
	"encoder.down.3.block.0.conv2.bias": "encoder.down_blocks.3.resnets.0.conv2.bias",
	"encoder.down.3.block.0.conv2.weight": "encoder.down_blocks.3.resnets.0.conv2.weight",
	"encoder.down.3.block.0.norm1.bias": "encoder.down_blocks.3.resnets.0.norm1.bias",
	"encoder.down.3.block.0.norm1.weight": "encoder.down_blocks.3.resnets.0.norm1.weight",
	"encoder.down.3.block.0.norm2.bias": "encoder.down_blocks.3.resnets.0.norm2.bias",
	"encoder.down.3.block.0.norm2.weight": "encoder.down_blocks.3.resnets.0.norm2.weight",
	"encoder.down.3.block.1.conv1.bias": "encoder.down_blocks.3.resnets.1.conv1.bias",
	"encoder.down.3.block.1.conv1.weight": "encoder.down_blocks.3.resnets.1.conv1.weight",
	"encoder.down.3.block.1.conv2.bias": "encoder.down_blocks.3.resnets.1.conv2.bias",
	"encoder.down.3.block.1.conv2.weight": "encoder.down_blocks.3.resnets.1.conv2.weight",
	"encoder.down.3.block.1.norm1.bias": "encoder.down_blocks.3.resnets.1.norm1.bias",
	"encoder.down.3.block.1.norm1.weight": "encoder.down_blocks.3.resnets.1.norm1.weight",
	"encoder.down.3.block.1.norm2.bias": "encoder.down_blocks.3.resnets.1.norm2.bias",
	"encoder.down.3.block.1.norm2.weight": "encoder.down_blocks.3.resnets.1.norm2.weight",

	"encoder.mid.attn_1.k.bias":          "encoder.mid_block.attentions.0.key.bias",
	"encoder.mid.attn_1.k.weight":        "encoder.mid_block.attentions.0.key.weight",
	"encoder.mid.attn_1.norm.bias":       "encoder.mid_block.attentions.0.group_norm.bias",
	"encoder.mid.attn_1.norm.weight":     "encoder.mid_block.attentions.0.group_norm.weight",
	"encoder.mid.attn_1.proj_out.bias":   "encoder.mid_block.attentions.0.proj_attn.bias",
	"encoder.mid.attn_1.proj_out.weight": "encoder.mid_block.attentions.0.proj_attn.weight",
	"encoder.mid.attn_1.q.bias":          "encoder.mid_block.attentions.0.query.bias",
	"encoder.mid.attn_1.q.weight":        "encoder.mid_block.attentions.0.query.weight",
	"encoder.mid.attn_1.v.bias":          "encoder.mid_block.attentions.0.value.bias",
	"encoder.mid.attn_1.v.weight":        "encoder.mid_block.attentions.0.value.weight",

	"encoder.mid.block_1.conv1.bias":   "encoder.mid_block.resnets.0.conv1.bias",
	"encoder.mid.block_1.conv1.weight": "encoder.mid_block.resnets.0.conv1.weight",
	"encoder.mid.block_1.conv2.bias":   "encoder.mid_block.resnets.0.conv2.bias",
	"encoder.mid.block_1.conv2.weight": "encoder.mid_block.resnets.0.conv2.weight",
	"encoder.mid.block_1.norm1.bias":   "encoder.mid_block.resnets.0.norm1.bias",
	"encoder.mid.block_1.norm1.weight": "encoder.mid_block.resnets.0.norm1.weight",
	"encoder.mid.block_1.norm2.bias":   "encoder.mid_block.resnets.0.norm2.bias",
	"encoder.mid.block_1.norm2.weight": "encoder.mid_block.resnets.0.norm2.weight",
	"encoder.mid.block_2.conv1.bias":   "encoder.mid_block.resnets.1.conv1.bias",
	"encoder.mid.block_2.conv1.weight": "encoder.mid_block.resnets.1.conv1.weight",
	"encoder.mid.block_2.conv2.bias":   "encoder.mid_block.resnets.1.conv2.bias",
	"encoder.mid.block_2.conv2.weight": "encoder.mid_block.resnets.1.conv2.weight",
	"encoder.mid.block_2.norm1.bias":   "encoder.mid_block.resnets.1.norm1.bias",
	"encoder.mid.block_2.norm1.weight": "encoder.mid_block.resnets.1.norm1.weight",
	"encoder.mid.block_2.norm2.bias":   "encoder.mid_block.resnets.1.norm2.bias",
	"encoder.mid.block_2.norm2.weight": "encoder.mid_block.resnets.1.norm2.weight",

	# Handle weird Von Platen:tm: keys
	"decoder.mid_block.attentions.0.to_k.bias":       "decoder.mid_block.attentions.0.key.bias",
	"decoder.mid_block.attentions.0.to_k.weight":     "decoder.mid_block.attentions.0.key.weight",
	"decoder.mid_block.attentions.0.to_out.0.bias":   "decoder.mid_block.attentions.0.proj_attn.bias",
	"decoder.mid_block.attentions.0.to_out.0.weight": "decoder.mid_block.attentions.0.proj_attn.weight",
	"decoder.mid_block.attentions.0.to_q.bias":       "decoder.mid_block.attentions.0.query.bias",
	"decoder.mid_block.attentions.0.to_q.weight":     "decoder.mid_block.attentions.0.query.weight",
	"decoder.mid_block.attentions.0.to_v.bias":       "decoder.mid_block.attentions.0.value.bias",
	"decoder.mid_block.attentions.0.to_v.weight":     "decoder.mid_block.attentions.0.value.weight",

	"encoder.mid_block.attentions.0.to_k.bias":       "encoder.mid_block.attentions.0.key.bias",
	"encoder.mid_block.attentions.0.to_k.weight":     "encoder.mid_block.attentions.0.key.weight",
	"encoder.mid_block.attentions.0.to_out.0.bias":   "encoder.mid_block.attentions.0.proj_attn.bias",
	"encoder.mid_block.attentions.0.to_out.0.weight": "encoder.mid_block.attentions.0.proj_attn.weight",
	"encoder.mid_block.attentions.0.to_q.bias":       "encoder.mid_block.attentions.0.query.bias",
	"encoder.mid_block.attentions.0.to_q.weight":     "encoder.mid_block.attentions.0.query.weight",
	"encoder.mid_block.attentions.0.to_v.bias":       "encoder.mid_block.attentions.0.value.bias",
	"encoder.mid_block.attentions.0.to_v.weight":     "encoder.mid_block.attentions.0.value.weight",
}

known_keys = {
	"decoder.conv_in.bias": True,
	"decoder.conv_in.weight": True,
	"decoder.conv_out.bias": True,
	"decoder.conv_out.weight": True,
	"encoder.conv_in.bias": True,
	"encoder.conv_in.weight": True,
	"encoder.conv_out.bias": True,
	"encoder.conv_out.weight": True,
	"post_quant_conv.bias": True,
	"post_quant_conv.weight": True,
	"quant_conv.bias": True,
	"quant_conv.weight": True,
}

for key in patch_list.keys():
	known_keys[patch_list[key]] = True
	known_keys[key] = True

do_not_copy = {
	"model_ema.decay": True,
	"model_ema.num_updates": True,
}

new_vae = {}
for key in vae_keys:
	if key in patch_list and key in known_keys:
		new_vae[patch_list[key]] = old_vae[key].clone().detach()
		#print(f"{key} > {patch_list[key]}")
		#print(f"{key}: {old_vae[key].size()}")
	elif key not in do_not_copy and key in known_keys:
		new_vae[key] = old_vae[key].clone().detach()
		#print(f"{key}: {old_vae[key].size()}")
	else:
		print(f"bad: {key} {old_vae[key].size()}")


save_file(tensors=new_vae, filename=args.out)